# 操作系统交流会：Eunomia: 基于 eBPF 的云原生监控工具

放一下学校和名字

## 项目背景

### 1

当前，云原生技术主要是以容器技术为基础，围绕着 Kubernetes 的标准化技术生态，通过标准可扩展的调度、网络、存储、容器运行时接口来提供基础设施，同时通过标准可扩展的声明式资源和控制器来提供运维能力。在这样的背景下，大量公司都使用云原生和容器技术技术来开发运维应用。

正因为云原生技术带来了更多可能性，当前业务应用出现了微服务众多、多语言开发、多通信协议的特征，同时云原生技术本身将复杂度下移，给可观测性，以及容器的安全性问题带来了更多挑战。

### 2

扩展伯克利数据包过滤器 (eBPF) 是一种内核技术，它无需更改内核源代码或添加其他模块便可以运行用户植入内核中的程序。在可观测性领域，ebpf 的优势在于：

- 速度和性能。 eBPF 可以将数据包处理这项工作从内核空间转移到用户空间。
- 低侵入性。 当作为调试器时，eBPF 不需要停止程序来观察其状态。
- 安全。 程序被有效地沙箱化了，这意味着内核源代码仍然受到保护并保持不变。eBPF程序的验证步骤确保资源不会被运行无限循环的程序阻塞。
- 方便。 创建hook内核函数的代码比构建和维护内核模块的工作要少。
-  统一追踪。 eBPF 为我们提供了一个单一、强大且易于访问的流程跟踪框架，这增加了可见性和安全性。
-  可编程性。 使用 eBPF 有助于在不添加额外层的情况下增加环境的功能丰富性。由于代码直接在内核中运行，因此可以在 eBPF 事件之间存储数据，而不是像其他跟踪器那样转储数据。

## 项目目标

### 1 & 2

项目目标：

实现一个基于 eBPF 的轻量级，高性能云原生监控工具框架，以最大化利用 ebpf 来解决容器可观测性问题中遇到的各种困难，它应该具有如下特征：

- 尽可能多的向下采集容器相关进程的系统和网络可观测数据，向上采集容器相关应用的性能数据，并且和 docker、Kubernetes 元信息相关联；
- 高度模块化，可扩展：可以轻松地自定义和添加新的 ebpf 跟踪程序，同时提供丰富的教程和文档，以及源码分析，以帮助 ebpf 初学者能够快速上手 ebpf 开发；
- 支持插件、配置分发和热插拔：可以直接通过 http API 下发 ebpf 子程序的相关配置，随时控制 ebpf 跟踪程序的启动和停止，避免在不必要的时候进行过多的信息采集占据系统资源，也大大提高了运维的便捷性；也可以自己编写插件来实现数据处理和控制的自动化
- 轻量级：利用 libbpf 提供的 CO-RE(Compile Once – Run Everywhere) 技术，不需要配置繁琐的 BCC 工具链和依赖内核头文件，通过一个紧凑的二进制文件即可以完成部署，并且能够以很快的速度完成 ebpf 子程序的启动和停止，避免 ebpf 实时编译过程中资源占用量大、时间长等问题；
- 高性能：利用好 ebpf 的性能，将数据处理这项工作从用户空间转移到内核空间，避免大量的上下文切换开销；减少内存和 CPU 占用率，避免影响到正常的业务程序；
- 高可用：一般而言，可观测性工具需要比被观测系统至少可靠一个数量级；
- 和开源的云原生可观测性系统、生态进行对接：和例如 prometheus、OpenTelemetry 这样的知名可观测性工具相对接，解决数据存储、可视化等功能，实现完整的解决方案；
- 在可观测性的基础上，提供基础的安全告警、安全分析和基于 seccomp、capability 的限制能力；

这部分可以分两页来讲

## 项目功能设计

放一下我们的系统架构图，然后快速过：

### ebpf 跟踪能力：

Eunomia 向下采集容器相关进程的系统和网络可观测数据，向上采集容器相关应用的性能数据。我们预先提供了十几种 ebpf 跟踪点，覆盖各种方面：

- `process`: 进程执行与退出
- `files`: 文件读写
- `tcpconnect`: TCP 连接
- `syscall`: 系统调用
- `tcpconnlat`: TCP 连接延时
- `tcprtt` 跟踪 TCP RTT(round-trip time)，并绘制直方图
- `profile`: 定时采集堆栈跟踪样本, 并进行 On-CPU 性能分析; 支持C/C++/Rust等，也支持对 lua 虚拟机进行采样分析；
- `funclatency` 输出显示函数延迟（调用时间）的直方图*
- `bindsnoop` 跟踪执行 socket bind 的内核函数
- `sigsnoop` 跟踪进程收到的信号；
- `opensnoop` 跟踪 open() 系统调用，并获取文件路径等信息
- `mountsnoop` 跟踪 mount() 和 umount 系统调用
- `memleak` 跟踪和匹配内存分配和释放请求*
- `oomkill`: 跟踪 Linux 内存不足 (OOM) 终止
- `syscount`: 追踪慢系统调用并进行统计

我们参考了 bcc/libbpf-tools 定制实现了我们自己的 ebpf 追踪器，所有的 ebpf 跟踪点都可以通过 pid、namespace、cgroups 等信息和 docker、Kubernetes 元信息相关联，完成端到端可观测数据的覆盖。

（可以分两页写）

### 模块化，可扩展

Eunomia 核心支持同时启动多个跟踪器，每个跟踪器可以作为一个独立的线程，也可以作为一个独立的进程运行，和 Eunomia 核心通过 pipe 等进程间通信机制进行通信；得益于 C++ 的面向对象的能力，只需继承一个基类，修改十几行代码和配置文件，即可添加自定义的、使用 C/C++ 编写的 ebpf 子程序；您也可以把 Eunomia 直接当做 C++ 依赖库进行开发；

正是得益于这种能力，我们已经预先从 bcc/libbpf-tools 中参考、修改、移植了不少现成的 ebpf 跟踪器过来；同时我们也积极给上游 BCC 仓库进行反馈，提了 PR 并被合并了进去；

我们有关于如何移植或添加新的 ebpf 程序的文档：

> 我等等写一下，这里可以放链接和截图

这里也可以点进去描述一下：

### ebpf 文档和教程

目前关于 ebpf 的资料还相对零散且过时，这也导致了我们在前期的开发过程中走了不少的弯路。因此, 我们也提供了一系列教程，以及丰富的参考资料，旨在降低新手学习eBPF技术的门槛，试图通过大量的例程解释、丰富对 eBPF、libbpf、bcc 等内核技术和容器相关原理的认知，让后来者能更深入地参与到 ebpf 的技术开发中来。

我们上线了一个 Eunomia 的网站：https://yunwei37.github.io/Eunomia/，里面包含了所有项目相关的文档和 ebpf 教程，还有我们的代码 API、类图等等：

> 这里放五六张截图

### 支持插件、配置分发和热插拔

Eunomia 支持以服务的方式启动，对外提供 HTTP API，可以实现自定义插件、配置分发和热插拔：

API 类似如下：

```json
POST /start

{
    "name": "tcpconnect",  // trace all tcp connect latency
    "args": [
        "-d 6d11ea6f0731", // docker id
        "-c"               // count connects per src, dest, port
        ],
    "export_handlers": [
        {
            "name": "stdout",
            "args": []
        },
        {
            "name": "prometheus",
            "args": [
                "eunomia_tcp_v4_counter"    // export as eunomia_tcp_v4_counter
            ]
        }
    ]
}
```

我们可以使用 API 热插拔控制特定 ebpf 跟踪点的启动、停止、获取对应 ebpf 跟踪点信息等；还可以通过配置分发，一次性完成大规模集群的配置；

### 轻量级

1. libbpf base

BCC 依靠运行时间汇编，将整个大型 LLVM/Clang 库带入并嵌入其中。这有许多后果，所有这些都不太理想：

- 编译过程中资源利用量大;
- 依赖于内核头包，该包必须安装在每个目标主机上;

文章BPF Portability and CO-RE 指出，为了提高BPF程序的便携性，即在不同内核版本上正常工作，而无需为每个特定内核重新编译的能力，社区提出了一个称为BPF CO-RE(Compile Once – Run Everywhere)的解决方案。

Libbpf+BPF CO-RE的理念是，BPF程序与任何"正常"用户空间程序没有太大区别：它们应该汇编成小型二进制文件，然后以紧凑的形式进行部署，以瞄准主机。Libbpf 扮演 BPF 程序装载机的角色，执行平凡的设置工作（重定位、加载和验证 BPF 程序、创建 BPF map、连接到 BPF 挂钩等），让开发人员只担心 BPF 程序的正确性和性能。这种方法将开销保持在最低水平，消除沉重的依赖关系，使整体开发人员体验更加愉快。

2. 基于 libbpf

不想安装一大堆 BCC 或者 systemTap 的环境？无法使用内核模块？因为网络不好镜像拉不下来？得益于 Libbpf + BPF CO-RE（一次编译，到处运行）的强大性能，仅需安装一个 agent 就可以收集这台主机所有相关的系统数据，最小仅需约 4MB 即可在支持的内核上或容器中启动跟踪，避免繁琐的依赖项和配置项；也可以通过镜像打包 Prometheus & Grafana 等监控可视化工具, 一站式开箱即用。

### 高性能、低开销

- 得益于 ebpf 的可编程特性，Eunomia 直接在内核中使用 eBPF 执行过滤、聚合、度量统计和直方图收集，避免向用户空间 agent 发送大量的低信号事件，大大减少了系统的开销；例如，我们对 opensnoop、 tcpconnlat、syscall 等工具，都进行了更进一步的优化，在内核中根据时间间隔、次数来统计，避免频繁上报事件；
- 在内核和用户态之间使用 map 和环形缓冲区进行通信
- 此外，Eunomia 使用了 C/C++ 高效的数据结构和多线程分析处理，以提供高效和快速的数据收集手段，在大多数情况下仅使用不到 2% 的 CPU 和少量内存。

1. 对正常业务的影响：内存、CPU 占用率等


2. 每秒事件统计

### 可用性：

- 一般而言，可观测性工具需要比被观测系统至少可靠一个数量级。我们启用了 github CI 进行自动构建,并 使用了大量的静态分析和动态分析工具，如 clang tidy、cppcheck、AddressSanitizer、ThreadSanitizer、Clang Static Analyzer 等，同时进行了大量的测试以增强系统的可用性以及稳定性；我们也通过静态分析和动态分析发现了一点 BCC 仓库中工具实现的缺陷，并反馈给了上游；
- 我们推荐将新增的自定义的 ebpf 跟踪器及 C 语言辅助程序，使用额外的进程运行，这样即使它意外崩溃，也不会影响到 eunomia 核心服务；

### 生态和系统集成

我们原生实现了 prometheus 的 exporter，也可以作为 OpenTelemetry 的 agent 使用；可以通过 prometheus、grafana 实现可视化和开箱即用的告警功能：

截图

## 和其他开源可观测性工具对比

### bcc

### prometheus ebpf exporter

## 总结